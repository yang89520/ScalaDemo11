





## 一、数仓分层

- 原始数据通过了flume-->kafka–>flume–>hdfs(日志)
- mysql–>hdfs(业务)
- 存放到了hdfs的gmall/[db|log]目录下，具体参考数据采集

### 1.数仓的分层

​		![image-20210609085842848](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210609085842848.png)

数仓的设计通常是3+1分层，ODS层、DW层、APP层、维度层

#### ODS层

- 此层最接近数据源，数据源长得数据经过抽取、转换、传输之后装入本层。

- 一般为了考虑后续的数据追溯的问题，此层的数据不建议进行过多的数据清洗工作，直接接入源数据即可。

- 至于数据的去噪、去重、异常值处理等过程可以放在后面的DWD层来做。

#### DW层

数据公共层

是数仓建设的重点，一般是日志子表和一些宽表，主要完成数据的清洗、转换等。

一般又可以分为DWD层、DWM层、DWS层

#### APP层



#### 维度层



DWS很多公司将DWS和DWT合在一起：为ADS层应用服务

DWS每日统计，最好以日分区

DWT累积统计

特点：

- 先将需求按照维度分类，
- 比如：
  -  							



什么时候建分区表，什么时候不建？

​		分区表创建的意义是为了分散数据！在hive中一个表的数据是存放在表目录中，如果有分区字段，数据被分散到多个表目录下的分区目录！ Hive 在计算时，使用MR模型，MR会通过解析sql语句，读取指定表目录中的数据！	如果在sql中使用 了分区字段进行过滤 ：  where  分区字段名= 分区字段值，程序只读取指定的分区目录，而不是全表扫描！ 

​		a) 数据量大需要分散数据，避免全表扫描！    被动分区！

​		b) 经常需要过滤查询的字段，可以设置为分区字段！也就要求表是分区表！

​				a业务：  select from a where age=xxx

​				b业务：  select from a where age=xxxx

​				c业务：  select from a where age=xxxx

​			就将age设置为分区字段，提高效率！

​			主动分区！

#### 分层如何理解

> ​		D：大概率 Data
>
> ​		W： 大概率 warehouse[仓库]
>
> ​		S :  大概率Store[存储]

- O（original 原始）DS： **原始数据层**。采集的数据不做任何处理，直接导入！
- DWD(detail 明细)： **明细数据层**。将采集的原始数据从ods层，将事实表的明细展开。
  - ETL,脱敏处理
-  DML（dimension 维度）： **维度数据层**。将采集的原始数据从ods层，将维度表的数据，先进行分类，分类后，同一类别的维度表的所有数据，会汇总到一张维度表中，称为`降维`(维度表数量的减少)。 
- DWS（service 服务）： **数据服务层**  。将ADS层的需求按照维度划分，每个维度的指标都每日聚合统计！
- DWT（topic 主题） ： **数据主题层**。  将ADS层的需求按照维度划分，每个维度的指标都每日累积（将当日的聚合结果和历史累积的结果，进一步合并）统计！
- A(application 应用)DS： **应用数据层** 。  为客户提供成品的数据。

#### 分层的使用

严格根据分层的理论限制数据的导入！

<img src="https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210609184954055.png" alt="image-20210609184954055" style="zoom:80%;" />

> 采集的原始数据------------->ODS层
>
> ODS层	------------->事实表------------> DWD
>
> ​				 ------------>维度表------------>DIM
>
>  DWD|DIM ------------------>DWS 
>
> DWS -------------------->DWT
>
> DWD(取决于建模)|DWS|DWT------------------>ADS

#### 导入数据的时候什么时候创建分区索引表？什么时候不创建？

- 原则：
  - 对经常需要过滤的字段建分区索引
  - 因为索引会降低插入的速度，当一个表很大的时候，首先考虑建分区，若有必要在分区上建索引

- 分区表创建的意义是为了分散数据！在hive中一个表的数据是存放在表目录中，如果有分区字段，数据被分散到多个表目录下的分区目录！ Hive 在计算时，使用MR模型，MR会通过解析sql语句，读取指定表目录中的数据！	如果在sql中使用 了分区字段进行过滤 ：  where  分区字段名= 分区字段值，程序只读取指定的分区目录，而不是全表扫描！ 

​		a) 数据量大需要分散数据，避免全表扫描！    被动分区！

​		b) 经常需要过滤查询的字段，可以设置为分区字段！也就要求表是分区表！

​				a业务：  select from a where age=xxx

​				b业务：  select from a where age=xxxx

​				c业务：  select from a where age=xxxx

​			就将age设置为分区字段，提高效率！

​			主动分区！

### 2.为什么要分层

①复杂问题简单化，方便维护

- 每层处理简单任务，出现问题方便定位

②减少相同需求的重复开发

- 规范数据分层，通过中间层数据，增加了数据的复用性

③隔离原始（敏感）数据

- 使真实的数据和统计分析数据解耦



### 3.数据集市和数据仓库

#todo 更多对比

数据集市是**微型**的数据仓库，主要面向某个部门或业务线！

数据仓库是面向整个公司！

### 4.数仓命名规范

#### 4.1表命名

> 哪一层的表_(性质)_表名_后缀

- 哪一层的表： ODS,DWD,DWS,DWT,ADS

- 性质： 
  - fact: 事实表
  - dim: 维度表

- 后缀： 
  - 如果是临时表，加tmp
  - 如果是用户行为数据，加log



#### 4.2脚本命名

> 数据源\_to\_目标_db/log.sh

例如： 

```shell
#脚本负责将用户行为数据，从HDFS导入到ODS层：  
hdfs_to_ods_log.sh

#脚本负责将用户业务数据，从ODS导入到DWD层：  
ods_to_dwd_db.sh
```

#### 4.3表字段命名

- Ø **数量**类型为bigint
- Ø **金额**类型为decimal(16, 2)，表示：16位有效数字，其中小数部分2位
- Ø **字符串**(名字，描述信息等)类型为string
- Ø **主键外键**类型为string
- Ø **时间戳**类型为bigint

## 二、数仓理论

### 1.范式

范式在设计一张表时需要遵守的规范和模式！

一般是在业务建表的时候遵循的模式



#### 范式优缺点

- **优点**

采用范式，可以**`降低数据的冗余性`**。

为什么要降低数据冗余性？

（1）十几年前，磁盘很贵，为了减少磁盘存储。

（2）以前没有分布式系统，都是单机，只能增加磁盘，磁盘个数也是有限的

（3）一次修改，需要修改多个表，很难保证数据一致性

- **缺点**

范式的缺点是获取数据时，需要通过**Join拼接**出最后的数据。

#### 没有范式可能会造成

​			①数据存储的冗余

​		    ②更新数据时，**`数据的不一致性`**！

遵守范式，可以解决上述问题！

#### 范式分类

- 第一范式(1NF)
- 第二范式(2NF)
- 第三范式(3NF)
- 巴斯-科德范式(BCNF)、第四范式(4NF)、第五范式(5NF)

重点掌握前三个

范式和函数依赖紧密相连

> 为什么说现在关系建模不一定要严格遵循三范式？

- **第一第二第三范式是递进依赖关系**，范式遵循的级别越高，冗余就会越少。
- 同时也就伴随着join的次数的增加，查询性能降低
- 由于现在很多关系数据库也能组成一定规模的集群
- 所以也可能不遵循三范式

##### 函数依赖

![image-20210610011522506](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210610011522506.png)

##### 三种范式

![image-20210610011648474](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210610011648474.png)

![image-20210610011815359](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210610011815359.png)

![image-20210610011903547](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210610011903547.png)

### 2.关系建模和维度建模

数据处理大致可分为两类：OLTP  OLAP

OLTP:

- 关系型数据库，传统增删改查，日常的事务处理

OLAP:

- 数据仓库，复杂的分析操作，侧重决策

| **对比属性** | **OLTP**                   | **OLAP**                   |
| ------------ | -------------------------- | -------------------------- |
| **读特性**   | 每次查询只返回少量记录     | 对大量记录进行汇总         |
| **写特性**   | 随机、低延时写入用户的输入 | 批量导入                   |
| **使用场景** | 用户，Java EE项目          | 内部分析师，为决策提供支持 |
| **数据表征** | 最新数据状态               | 随时间变化的历史状态       |
| **数据规模** | GB                         | TB到PB                     |

**`关系建模(E-R建模)`**

- 用于关系型数据的表模型设计！
  - 特征： ①强调数据的治理(精细)

​								  ②强掉数据的整合

​								  ③保证消除数据的冗余和保证数据强一致性

- 看起来比较复杂！
- **弊端**： 如果要求业务的全部信息，需要进行多次的关联！

> 关系模型严格遵循第三范式（3NF），数据冗余程度低，数据的一致性容易得到保证。由于数据分布于众多的表中，查询会相对复杂，在大数据的场景下，查询效率相对较低。

**`维度建模：`** 

- 主要**面向业务**！**不在乎冗余和强一致性**！业务的实现怎么方便怎么来！
  - 特征：
    - 事实表 + 若干维度表
    - 不太遵循范式！将多个维度，**降维**为一个维度！方便和事实表进行关联！减少关联次数！
- 主要用在大数据的应用场景上！

> 维度模型以数据分析作为出发点，不遵循三范式，故数据存在一定的冗余。维度模型面向业务，将业务用事实表和维度表呈现出来。表结构简单，故查询简单，查询效率较高。

### 3.维度表和事实表

**事实表：**

在表中记录一个事实（**已经发生的事件，动作，度量**）的信息！

每一个事实表的行包括：

- 具有可加性的数值型的**度量值**
  - 度量值（次数、个数、件数、金额，可以进行累加）
- 与维表相连接的外键，通常具有两个和两个以上的外键。

例如：

> 我今天中午去津味源吃了一份20元的套餐，3个菜
>
> 我和同桌今天中午去津味源吃了一份20元的套餐，3个菜
>
> 事实表基本都有以下元素：
>
> ​	人物  时间  地点          度量
>
> ​	who  when  where   



**维度表：**

用来描述**事实中的部分属性**，一般都是一些名词

维表对应现实世界中的对象或者概念。  例如：用户、商品、日期、地区等



**事实表和维度表特点**

事实表

- 非常大
- 字段相对维度表窄(主要是外键和度量值)
- 经常变化，每天新增数据很多

维度表

- Ø 维表的范围很宽（具有多个属性、列比较多）
- Ø 跟事实表相比，行数相对较小：通常< 10万条
- Ø 内容相对固定：编码表



### 4.维度建模的模型

**`星型模型：`** 

- 维度表直接关联在事实表上！
- 当查询某个维度时，最多只需要关联(join)一次！
- 维度建模中使用的最多的！



**`雪花模型：`** 

- 类似关系建模！存在维度表间接关联在事实表上！
- 当查询某个维度时，可能需要关联(join)多次！



**`星座模型：`** 

- 本质上也是星型或雪花模型！是星型模型的一个变种！
- 可能有多个事实表！维度表还是直接关联在事实表上！存在多个事实表共有一个维度表！

数仓建模中常使用星座建模，即多张事实表公用维度表，事实表和维度表之间一般采用星型建模，能有效减少shuffle



![image-20210609110048290](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210609110048290.png)

![image-20210609110111737](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210609110111737.png)

### 5.事实表的分类

#### 事务型事实表

- 如果一类事实，一旦发生，就不会变化(insert 不允许update)。记录这类事实的表，称为事务型事实表！

​							这个表的特点是表中的数据**`只会不断新增，不会发生修改`**！

​							举例： 支付信息详情表！

​							事务型事实表  在同步数据时，只同步当天新增的数据！



#### 周期型快照事实表

- 如果某个事实，在一个周期内会不断发生变化，只需要记录在周期结束时，事实的状态，此时这类表称为 周期型快照事实表！
- **`就是记录一个周期内的最后一个状态`**

​							举例： 记录一个人身高生长的事实

| 人   | 时间     | 身高 |
| ---- | -------- | ---- |
| jack | 2020-1-1 | 60   |
| jack | 2021-1-1 | 60   |
| jack | 2022-1-1 | 80   |

周期型快照事实表: 事实的记录会有周期，重视周期结束时，事实的状态（结果）！



重点：确定记录的周期是多久！



粒度： 一个人在一个周期 ，身高的事实

在hive中更新方式： 

​		全量表 ：   insert into

​		分区表：   确定查询时，经常使用什么字段过滤！因为如果分区和查询过滤字段不一致，就需要全表扫描，所以**`按什么过滤就按什么分区`**

​								例如采取name作为分区字段！

​							  insert into ，造成大量的小文件！

​							



#### 累积型快照事实表

如果一个事实，在其生命周期内，不断变化！只记录在某些时间点的状态变化，且可以查看一个状态的累积变化趋势，称这类表为累积型快照事实表！

举例： 记录一个人身高生长的事实

| 人   | 出生 | 3岁时 | 12岁（青春期） | 18（成年时） | 22（成人时） |      |
| ---- | ---- | ----- | -------------- | ------------ | ------------ | ---- |
| jack | 40   | 70    | 120            | 180          | 183          |      |
| tom  | 30   | 60    |                |              |              |      |

| 人   | 时间点 | 身高 |
| ---- | ------ | ---- |
| jack | 出生   | 40   |
| tom  | 出生   | 40   |
| jack | 三岁   | 70   |
| tom  | 三岁   | 60   |



列转行实现了每一行是一个过滤，本质就是

这种情况下，需要不断更新，hive本身不允许修改数据，一种方式是使用事务，另一种是查询旧表和新表和并，用覆盖写实现修改数据。

> 在hive中更新方式：
>
> - update操作
>
> - hive默认不支持update!
>   - insert overwrite (select  old  +   new )

粒度： 一个人的身高的事实

一个人的所有身高事实

一个人在一个周期内的所有身高事实







分类是为了，总结同一类型表导入数据的方式！方便向表中导入数据！



### 6.数仓项目的开发

互联网运营：  收集企业要求收集的指标

产品经理：  决定是否需要增加功能，修改完善已有的功能

项目经理：  管理。统筹，协调

架构师： 架构设计

开发工程师： 负责项目的编码，实施。

​							HQL

运维：        项目大数据平台的运行和维护

建模工程师：  建表和随着业务指标的变化调整模型



## 三、数仓环境搭建

Hive on Spark

### 教学版本

#### 1.环境

要求： ①安装Spark，配置Spark on YARN

​						在Hive所在机器安装！

​						配置和导出SPARK_HOME到全局变量！

​			②安装Hive

​					安装和Spark对应版本一起编译的Hive

​					配置元数据到Mysql，修改对中文注释的支持！

​			③配置HIve on spark

```xml
<!--Spark依赖位置-->
<property>
    <name>spark.yarn.jars</name>
    <value>hdfs://hadoop102:8020/spark-jars/*</value>
</property>
  
<!--Hive执行引擎-->
<property>
    <name>hive.execution.engine</name>
    <value>spark</value>
</property>

<!--Hive和spark连接超时时间-->
<property>
    <name>hive.spark.client.connect.timeout</name>
    <value>10000ms</value>
</property>
```

​		上传纯净版的spark的jars中的jar包到/spark-jars

#### 2.配置容量调度器为多队列

编辑$HADOOP_HOME/etc/hadoop/capacity-schdualer.xml

```xml
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>

   <!-- 容量调度器最多可以容纳多少个job-->
  <property>
    <name>yarn.scheduler.capacity.maximum-applications</name>
    <value>10000</value>
    <description>
      Maximum number of applications that can be pending and running.
    </description>
  </property>

  <!-- 当前队列中启动的MRAppMaster进程，所占用的资源可以达到队列总资源的多少
		通过这个参数可以限制队列中提交的Job数量
  -->
  <property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>0.5</value>
    <description>
      Maximum percent of resources in the cluster which can be used to run 
      application masters i.e. controls number of concurrent running
      applications.
    </description>
  </property>

  <!-- 为Job分配资源时，使用什么策略进行计算
  -->
  <property>
    <name>yarn.scheduler.capacity.resource-calculator</name>
    <value>org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator</value>
    <description>
      The ResourceCalculator implementation to be used to compare 
      Resources in the scheduler.
      The default i.e. DefaultResourceCalculator only uses Memory while
      DominantResourceCalculator uses dominant-resource to compare 
      multi-dimensional resources such as Memory, CPU etc.
    </description>
  </property>

   <!-- root队列中有哪些子队列-->
  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default,hive</value>
    <description>
      The queues at the this level (root is the root queue).
    </description>
  </property>

  <!-- root队列中default队列占用的容量百分比
		所有子队列的容量相加必须等于100
  -->
  <property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>30</value>
    <description>Default queue target capacity.</description>
  </property>
  
  <property>
    <name>yarn.scheduler.capacity.root.hive.capacity</name>
    <value>70</value>
    <description>Default queue target capacity.</description>
  </property>
  
  

    <!-- 队列中用户能使用此队列资源的极限百分比
  -->
  <property>
    <name>yarn.scheduler.capacity.root.default.user-limit-factor</name>
    <value>1</value>
    <description>
      Default queue user limit a percentage from 0.0 to 1.0.
    </description>
  </property>
  
   <property>
    <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
    <value>1</value>
    <description>
      Default queue user limit a percentage from 0.0 to 1.0.
    </description>
  </property>
 

  <!-- root队列中default队列占用的容量百分比的最大值
  -->
  <property>
    <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
    <value>100</value>
    <description>
      The maximum capacity of the default queue. 
    </description>
  </property>
  
   <property>
    <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
    <value>100</value>
    <description>
      The maximum capacity of the default queue. 
    </description>
  </property>
  
  

    <!-- root队列中每个队列的状态
  -->
  <property>
    <name>yarn.scheduler.capacity.root.default.state</name>
    <value>RUNNING</value>
    <description>
      The state of the default queue. State can be one of RUNNING or STOPPED.
    </description>
  </property>
  
    <property>
    <name>yarn.scheduler.capacity.root.hive.state</name>
    <value>RUNNING</value>
    <description>
      The state of the default queue. State can be one of RUNNING or STOPPED.
    </description>
  </property>

  
  <!-- 限制向default队列提交的用户-->
  <property>
    <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
    <value>*</value>
    <description>
      The ACL of who can submit jobs to the default queue.
    </description>
  </property>
  
  <property>
    <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
    <value>*</value>
    <description>
      The ACL of who can submit jobs to the default queue.
    </description>
  </property>
  

  <property>
    <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
    <value>*</value>
    <description>
      The ACL of who can administer jobs on the default queue.
    </description>
  </property>
  
  <property>
    <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
    <value>*</value>
    <description>
      The ACL of who can administer jobs on the default queue.
    </description>
  </property>
  
 

  <property>
    <name>yarn.scheduler.capacity.node-locality-delay</name>
    <value>40</value>
    <description>
      Number of missed scheduling opportunities after which the CapacityScheduler 
      attempts to schedule rack-local containers. 
      Typically this should be set to number of nodes in the cluster, By default is setting 
      approximately number of nodes in one rack which is 40.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.queue-mappings</name>
    <value></value>
    <description>
      A list of mappings that will be used to assign jobs to queues
      The syntax for this list is [u|g]:[name]:[queue_name][,next mapping]*
      Typically this list will be used to map users to queues,
      for example, u:%user:%user maps all users to queues with the same name
      as the user.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.queue-mappings-override.enable</name>
    <value>false</value>
    <description>
      If a queue mapping is present, will it override the value specified
      by the user? This can be used by administrators to place jobs in queues
      that are different than the one specified by the user.
      The default is false.
    </description>
  </property>

</configuration>

```

分发到集群，重启YARN！

如果出现了hadoop集群无法关闭是因为hadoop的pid默认放在/tmp目录下

我们需要收容杀死yarn的rm和nm再重启yarn即可。

在hive-site.xml中添加：

```xml
<property>
    <name>mapreduce.job.queuename</name>
    <value>hive</value>
</property>

```

重启Hive!

打开hive客户端提交job测试。

### 2.x和3.x通用版

① 安装配置好sparkOnYarn

② 将spark的spark-defaults.conf拷贝到hive的conf目录

③ 修改hive的配置文件，主要有三个

- hive-site.xml 对hiveOnSpark的hdfs jar包目录做配置，hive单独队列配置，spark延迟链接配置
- hive-env.sh 配置hive相关的环境依赖
- spark-defaults.conf spark的日志聚集保存路径，spark的相关配置

④ 将spark的除开hive相关的jar包上传到hdfs集群

⑤ 创建日志文件夹

hive-site.xml

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <!--<value>jdbc:mysql://hadoop102:3306/metastore?useSSL=false</value>-->
        <value>jdbc:mysql://hadoop102:3306/sparkmetastore?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF8&amp;useSSL=false</value>   
 </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>Yang89520..</value>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/sparkhive/warehouse</value>
    </property>

    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>

    <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
    </property>

    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>hadoop102</value>
    </property>

    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>
    
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
    <!-- tez配置 -->
<!--    
<property>
        <name>hive.execution.engine</name>
        <value>tez</value>
    </property>
-->
<!-- spark连接的超时时间 -->  
<property>
    <name>hive.spark.client.server.connect.timeout</name>
    <value>300000</value>
  </property>
  <!-- Spark2 依赖库位置，在YARN 上运行的任务需要从HDFS 中查找依赖jar 文件
         ${fs.defaultFS} 在Hadoop3 的core-site.xml 中设置，表HDFS 根路径，这里引用即可 -->
  <property>
    <name>spark.yarn.jars</name>
    <!--<value>hdfs://hadoop102:9820/spark2-jars/*</value>-->
    <value>${fs.defaultFS}/spark3-jars/*</value>
  </property>
  
  <!-- Hive3 执行引擎设为spark -->
  <property>
    <name>hive.execution.engine</name>
    <value>spark</value>
  </property>

  <!-- Hive3 和Spark2 连接超时时间 -->
  <property>
    <name>hive.spark.client.connect.timeout</name>
    <value>10000ms</value>
  </property>

  <!-- 将原Spark 配置文件spark-defaults.conf 的内容也可以写在hive-site.xml 这里
         例如：原property 形式的内容spark.driver.port    36666 可转换成以下xml 配置形式
  或者参考下一节内容创建hive-3.1.2/conf/spark-defaults.conf 以影响Hive on Spark -->
  <!--
         <property>
    <name>spark.driver.port</name>
    <value>36666</value>
  </property>
  -->
  <!-- 提交任务到指定队列 -->
<property>
      <name>mapreduce.job.queuename</name>
      <value>hive</value>
  </property>
</configuration>
```



hive-env.sh

```shell
# 新增SPARK_HOME 必须是without-hadoop 的纯净版
export SPARK_HOME=/opt/module/spark/spark-yarn

# Set HADOOP_HOME to point to a specific hadoop install directory
export HADOOP_HOME=/opt/module/hadoop-3.0.0



# 以下是tez的配置文件
# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/opt/module/hive/conf

# Folder containing extra libraries required for hive compilation/execution can be controlled by:
export TEZ_HOME=/opt/module/tez    #是你的tez的解压目录
export TEZ_JARS=""
for jar in `ls $TEZ_HOME |grep jar`; do
    export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar
done
for jar in `ls $TEZ_HOME/lib`; do
    export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar
done

export HIVE_AUX_JARS_PATH=/opt/module/hadoop/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS

```

spark-defaults.conf

```xml
spark.master              yarn
# 启用日志聚合
spark.eventLog.enabled    true
# 保存日志的HDFS 路径
spark.eventLog.dir        hdfs://hadoop102:9820/spark3-history
# 或保存到HDFS HA 的目录，参考core-site.xml 的fs.defaultFS 属性
#spark.eventLog.dir        hdfs://hdp3cluster/spark2-history
spark.executor.memory     1g
spark.driver.memory       1g
```

在hdfs上创建两个文件目录

```shell
hadoop fs -mkdir /spark3-jars
hadoop fs -mkdir /spark3-history
```

自己编译纯净版的spark3jar包上传上去

解压出来

```shell
tar -zxvf spark-3.0.0-bin-without-hadoop -C .
```

删除这个目录下的jars目录和hive相关的jar包,再上传不带hive依赖的jar包

因为spark和hive版本不一致，spark3是兼容hive2.3的，目前用的spark3 所以不能使用内置的jar环境会冲突

```shell
hadoop fs -put ./* /spark3-jars
```

配置多队列支持

主要是配置hadoop的调度配置文件

#vim capacity-scheduler.xml

参考教学版本的容量调度器的多队列配置

**一些可能的问题：**

队列资源不足 

hadoop相关环境变量配置错误

必须指定的spark是spark-yarn模式

## 数仓分层搭建

### 数据建模规划



#### ODS层

![image-20210610211258839](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210610211258839.png)

![image-20210610211430318](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210610211430318.png)

针对HDFS上的用户行为数据(log)和业务数据(db)，我们如何规划处理？

（1）保持数据原貌不做任何修改，起到备份数据的作用。

（2）数据采用压缩，减少磁盘存储空间（例如：原始数据100G，可以压缩到10G左右）

（3）创建分区表，防止后续的全表扫描

#### DIM层和DWD层

建模展示：

![image-20210612221339642](https://gitee.com/Mryang_bast/typera/raw/master/hive_imgs/image-20210612221339642.png)

**`DIM和DWD层需`：**

- 要构建维度模型，采用**星型建模**
- 要构建呈现状态模型，采用**星座建模**

**`维度建模的步骤`：**

**`确定维度`**

- 在关系数据库中去挑选：`名词`、`描述相关`的表
- 谁、何处、何时等描述信息
- 可以根据应用层数据反推需要的维度，但一般拿尽可能多的维度

**`确定事实`**

**选择业务->申明粒度->确认维度->确认事实**【重点】

- 选择业务
  - 在所有业务系统中，筛选出**需要的**业务过程，比如`下单、支付、退款、物流业务`，`一条`业务线对应`一张事实表`
  - 严格来说现在用不到这个业务数据也要放到维度模型里，避免未来需要数据时没有数据可用。
  - 中小型公司：尽量所有业务过程都选择
  - 大公司(1000多张表)：选择和需求相关的业务过程
- 申明粒度
  - 粒度事实表中的**一行数据**
  - 越明细粒度越小，越聚合粒度越粗
  - 原则：尽可能选择粒度最小的数据
  - 为什么要选择粒度最小的数据？
    - 因为需要应对宽表DWS DWT和应用层ADS 的不断变化的业务需求
- 确认维度
  - 确认每张事实表和哪些维度有关系
  - 到底和哪些维度有关，需要从业务系统中确定
  - 比如支付事实：时间、用户、地区维度等
- 确定事实
  - 确定事实表的**度量值**

### ODS层

#### 建模

```sql
CREATE EXTERNAL TABLE ods_log (`line` string)
PARTITIONED BY (`dt` string) -- 按照时间创建分区
STORED AS -- 指定存储方式，读数据采用LzoTextInputFormat；
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'  --给insert 语句使用
LOCATION '/warehouse/gmall/ods/ods_log'  -- 指定数据在hdfs上的存储位置
;
```

Managed Table:  hive表不仅负责管理元数据，还负责数据的管理。 删表时，将数据和元数据一起删除！

External Table :  hive不负责管理数据！  删表时，只删元数据！



分区表是为了分散数据，只需要在数据量大时，创建！



特征： ①外部表

​			 ②是分区表， 使用采集的数据的日期作为分区字段

​			 ③列的数量和类型如何确定？

​						 日志：  没有分隔符，1列，string

​						 业务：    使用\t分割，分割后是几列，ods表就设置几列，采集的每一列是什么类型，ods层的每一列设置对于的类型

​			④默认存放在hive的根目录  /user/hive/warehouse

​				可以通过location属性，制定表目录创建的位置

​			⑤文件格式： textfile + lzo 

​						select :  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'

​						不使用insert，无需修改 OUTPUTFORMAT，使用默认

-------------

Store as :   制定的是数据的存储格式。

​				   ods层的特点是采集的数据不做任何处理，直接导入到表。采集的数据文件是什么类型，表中放的数据文件格式就是什么类型！

​					采集：  textfile +  lzo压缩

​					表中的数据格式：    textfile +  lzo压缩

Hive :    select  xxx    from  表名

​						HQL-------------->Job(MR|spark)

​								输入(表中的数据)-------计算-------输出

​									MR：   InputFormat

​									Spark:      textFile(路径名)  --------> HadoopRdd 

​																	InputFormat(TextInputFormat)

​				  textfile +  lzo压缩 只能使用com.hadoop.mapred.DeprecatedLzoTextInputFormat 读取！

​					在建表时，需要设置一个参数INPUTFORMAT ，代表Job只要从当前这个表读取数据，请设置输入格式为com.hadoop.mapred.DeprecatedLzoTextInputFormat 。

​				 insert  into|overwrite table 表名  values | select 

​						HQL-------------->Job(MR|spark)

-------

InputFormat:  输入格式。    输入数据的文件格式！

​									.avi

​									.mp4

​									.word

​									.txt

​		默认：  TextInputFormat ----------->	.txt

​						AVIInputFormat------------>   .avi

​		                DBInputFormat(sqoop)-------------->  mysql的表



​		程序输入的数据的文件格式是什么，就需要设置InputFormat=对于的类



OutputFormat: 输出格式。Job最终数据写出的文件类型！



-------

想ODS层导入数据，会不会使用insert?

​		数据： /origin_data/gmall/db/activity_info/2021-06-08

​		表目录： /warehouse/gmall/ods/ods_activity_info/2021-06-08



​		load :  load  local  将本地的文件上传到HDFS的表目录

​					load  默认认为文件在HDFS，将数据从HDFS  移动(剪切) 到表目录



​		insert?   实现不了！实现极其困难！劝退！

​				insert  into  ods_activity_info  values( )    放弃

​					insert  into  ods_activity_info  select   放弃

​								

------------

总结：   建表时 INPUTFORMAT  和  OUTPUTFORMAT 都取决于表中数据存储的文件格式

​			INPUTFORMAT  :  select   

​			OUTPUTFORMAT ： insert 

​			

文件格式：    Hive 支持

TEXTFILE(默认)： 

​		INPUTFORMAT:org.apache.hadoop.mapred.TextInputFormat

​					可切。 默认以blocksize为每片最大值。

​      OUTPUTFORMAT: 	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	



com.hadoop.mapred.DeprecatedLzoTextInputFormat：

​			可切。 取决于index文件中的设置。没有index文件，不能切，将整个文件作为1片！



SequenceFile（过时）:



ORC：

InputFormat:        	org.apache.hadoop.hive.ql.io.orc.OrcInputFormat	 

​			可切。一个stripe是一片！

OutputFormat:       	org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat



Parquet: 					

InputFormat:        	org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat	 

​			可切。一个数据页是1片！ 256M

OutputFormat:       	org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat					



#### 1.ods_log

##### 1.1建表

```sql
drop table if exists ods_log;
CREATE EXTERNAL TABLE ods_log (`line` string)
PARTITIONED BY (`dt` string) -- 按照时间创建分区
STORED AS -- 指定存储方式，读数据采用LzoTextInputFormat；
  INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/warehouse/gmall/ods/ods_log'  -- 指定数据在hdfs上的存储位置
;
```



##### 1.2 导入数据

```sql
load data inpath '/origin_data/gmall/log/topic_log/2021-06-08' overwrite into table ods_log partition (dt='2021-06-08');
```



手动创建索引： 注意不是HQL，不能在Hive客户端允许！

```shell
hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /warehouse/gmall/ods/ods_log/dt=2021-06-08
```



#### 2.引号

主要看最外层是什么引号，如果最外侧是单引号，此时不支持脱义，$AAA, 不做解析！

如果最外侧是双引号，此时支持脱义，$AAA, 会引用AAA变量的值！



在HQL中，字符串，一律使用单引号，不要使用双引号。



#### 3.业务数据导入

业务数据，分为两类，一类是采集的全量的存量数据，另一类是每天导入的数据！



虽然分了两类，但是ODS的特点是不做任何处理！因此哪一天采集的数据，就放入哪一天的ODS的分区！



首日同步时，比每日同步多同步了2张表： base_province,base_region!



**解决思路**： 只需要编写一个脚本，每日同步和首日同步思路一样，都是直接load。 手动将 base_province,base_region导入到ods层。



手动导入：

```sql
load data inpath '/origin_data/gmall/db/base_province/2021-06-08' OVERWRITE into table ods_base_province;
load data inpath '/origin_data/gmall/db/base_region/2021-06-08' OVERWRITE into table ods_base_region;
```

### DMI层



flume文件不滚动的后缀是.tmp taildirsource是不会是变tmp后缀的文件

# 

